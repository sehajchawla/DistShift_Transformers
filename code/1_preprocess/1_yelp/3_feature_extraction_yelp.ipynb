{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading raw data from csv files provided by Shebuti Rayana\n",
    "reviews = pd.read_csv(\"yelp_all.csv\")\n",
    "restaurants = pd.read_csv(\"ProductIdMapping.csv\")\n",
    "metadata = pd.read_csv(\"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "familiar-lodging",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metadata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5e6563f0b581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"prodID\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"restaurantID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"fakeLabel\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Unnamed: 0\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"ID\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"fakeLabel\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"restaurantID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"productID\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"userID\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrestaurants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestaurants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Restaurant\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"restaurant\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ProductId\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"restaurantID\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metadata' is not defined"
     ]
    }
   ],
   "source": [
    "#Renaming columns to make the dataset more comprehensible\n",
    "metadata = metadata.rename(columns = {\"prodID\":\"restaurantID\", \"label\":\"fakeLabel\"})\n",
    "reviews = reviews.rename(columns = {\"Unnamed: 0\": \"ID\" , \"fakeLabel\":\"restaurantID\", \"productID\": \"userID\"})\n",
    "restaurants = restaurants.rename(columns = {\"Restaurant\":\"restaurant\",\"ProductId\":\"restaurantID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "#At this stage we combine the divided information into one single dataset. We include the restaurant information on the complete dataset and include columns for every review's rating and\n",
    "#the label indicating if that review was flagged as fake or not. To facilitate comprehension, the fakeLabel column was adapted to the following logic: 1 if review was flaged as fake, 0\n",
    "#otherwise.\n",
    "#NOTE: We performed several rounds of testing to determine the most efficient way to join the metadata information and the reviews themselves. The best alternative because of its simplicity\n",
    "#proved to be simply element-wise attribution. This is possible because rows in both csv files are exactly in the same order, which had been previously validated.\n",
    "complete_reviews = reviews.join(restaurants.set_index('restaurantID'), on='restaurantID')\n",
    "complete_reviews['fakeLabel'] = np.where(metadata['fakeLabel'] < 0, 1, 0)\n",
    "complete_reviews['rating'] = metadata['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One feature to be included in the model is the length of the review. Therefore, it is necessary to add the wordcount for every review\n",
    "complete_reviews['wordCount'] = complete_reviews['reviewText'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another feature is the sentiment analysis (positive or negative). Three different methods were used to determine the sentiment: Vader, within the Natural Language Toolkit framework, Blob\n",
    "#and Flair. The review text was fed into the pre-trained models and the corresponding sentiment (between -1 and 1) was saved in the data set.\n",
    "#NOTE: Flair yielded the most accurate results in a shorter version of the dataset (analyzing random samples by reading the reviews). However, it is the most expensive to run in terms of\n",
    "#computational resources. Future versions of the model will include sentiment data from Flair instead of Blob and Vader.\n",
    "complete_reviews['sentimentBLOB'] = complete_reviews['reviewText'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "complete_reviews['BLOBPosNeg'] = np.where(complete_reviews['sentimentBLOB'] < 0, 0, 1)\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "complete_reviews['sentimentVADER'] = complete_reviews['reviewText'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "complete_reviews['VADERPosNeg'] = np.where(complete_reviews['sentimentVADER'] < 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_reviews.to_csv('yelp_processed.csv', encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a file only with metadata (without the text reviews) to be added to an initial model. The text reviews are analyzed by a\n",
    "#separate module at this initial stage\n",
    "metadata_processed = complete_reviews.drop(columns=['ID','restaurantID','userID','reviewText','restaurant'])\n",
    "metadata_processed.to_csv('yelp_metadata_processed.csv', encoding = \"UTF-8\")\n",
    "\n",
    "#Standardizes numeric variables (rating and wordcount)\n",
    "scaler = StandardScaler()\n",
    "metadata_standardized = metadata_processed.copy()\n",
    "metadata_standardized['rating'] = scaler.fit_transform(metadata_standardized[['rating']])\n",
    "metadata_standardized['wordCount'] = scaler.fit_transform(metadata_standardized[['wordCount']])\n",
    "metadata_standardized.to_csv('yelp_metadata_standardized.csv', encoding = \"UTF-8\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
